#!/usr/bin/env python3
"""
æµè§ˆå™¨æˆæƒTwitteræŠ“å–å·¥å…·
ä¸“é—¨ä¸ºæµè§ˆå™¨cookiesç™»å½•è®¾è®¡çš„ç®€åŒ–ç‰ˆæœ¬
"""

import asyncio
import json
import os
import sys
from datetime import datetime
import logging
from cookies_manager import CookiesManager

# å¤„ç†ä»£ç†è®¾ç½®
def get_proxy_config():
    """è·å–ä»£ç†é…ç½® - è‡ªåŠ¨æ£€æµ‹å¸¸ç”¨ä»£ç†"""
    # é¦–å…ˆæ£€æŸ¥ç¯å¢ƒå˜é‡
    proxy_vars = ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY', 'all_proxy', 'ALL_PROXY']

    for var in proxy_vars:
        proxy_url = os.environ.get(var)
        if proxy_url:
            print(f"ğŸ”§ æ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ä»£ç†: {var}={proxy_url}")
            # è½¬æ¢socksä»£ç†æ ¼å¼
            if proxy_url.startswith('socks://'):
                proxy_url = proxy_url.replace('socks://', 'socks5://')
                print(f"ğŸ”„ è½¬æ¢ä»£ç†æ ¼å¼: {proxy_url}")
            return proxy_url

    # å¦‚æœæ²¡æœ‰ç¯å¢ƒå˜é‡ï¼Œè‡ªåŠ¨å°è¯•å¸¸ç”¨ä»£ç†
    print("ğŸ” æœªæ£€æµ‹åˆ°ä»£ç†ç¯å¢ƒå˜é‡ï¼Œå°è¯•å¸¸ç”¨ä»£ç†...")
    common_proxies = [
        'socks5://127.0.0.1:7897',  # ä½ çš„ä»£ç†ç«¯å£
        'socks5://127.0.0.1:1080',  # å¸¸ç”¨socks5ç«¯å£
        'socks5://127.0.0.1:7890',  # Clashé»˜è®¤ç«¯å£
        'http://127.0.0.1:8080',    # å¸¸ç”¨httpä»£ç†
    ]

    for proxy in common_proxies:
        print(f"ğŸ§ª å°è¯•ä»£ç†: {proxy}")
        return proxy  # ç›´æ¥è¿”å›ç¬¬ä¸€ä¸ªï¼ˆä½ çš„7897ç«¯å£ï¼‰

    return None

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logging.getLogger("httpx").setLevel(logging.WARNING)  # å±è”½httpxçš„INFOæ—¥å¿—
logger = logging.getLogger(__name__)

class BrowserAuthScraper:
    """åŸºäºæµè§ˆå™¨æˆæƒçš„TwitteræŠ“å–å™¨"""
    
    def __init__(self, cookies_file: str = "cookies.json"):
        self.cookies_file = cookies_file
        self.client = None
        self.user_info = None
        self.is_logged_in = False
    
    async def login_with_cookies(self) -> bool:
        """ä½¿ç”¨cookiesç™»å½•"""
        from twikit import Client

        if not os.path.exists(self.cookies_file):
            print(f"âŒ Cookiesæ–‡ä»¶ä¸å­˜åœ¨: {self.cookies_file}")
            print("è¯·å…ˆè¿è¡Œ: python cookie_helper.py guide")
            return False

        try:
            print(f"ğŸª åŠ è½½cookiesæ–‡ä»¶: {self.cookies_file}")

            # ä½¿ç”¨cookiesç®¡ç†å™¨
            cookies_manager = CookiesManager(self.cookies_file)
            if not cookies_manager.load_cookies():
                return False

            cookies = cookies_manager.get_cookies()

            # åˆ›å»ºå®¢æˆ·ç«¯ - ä½¿ç”¨ç³»ç»Ÿä»£ç†
            proxy_config = get_proxy_config()
            if proxy_config:
                print(f"ğŸ”§ åˆ›å»ºå®¢æˆ·ç«¯ (ä½¿ç”¨ä»£ç†: {proxy_config})...")
                self.client = Client('en-US', proxy=proxy_config)
            else:
                print("ğŸ”§ åˆ›å»ºå®¢æˆ·ç«¯ (æ— ä»£ç†)...")
                self.client = Client('en-US')
            self.client.set_cookies(cookies)

            # è®¾ç½®CSRFå¤´
            if 'ct0' in cookies:
                self.client.http.headers['x-csrf-token'] = cookies['ct0']
                print(f"ğŸ›¡ï¸ è®¾ç½®CSRFä»¤ç‰Œ: {cookies['ct0'][:20]}...")
            
            # éªŒè¯ç™»å½•çŠ¶æ€ - ç›´æ¥å°è¯•æœç´¢æ¨æ–‡
            print("ğŸ” éªŒè¯ç™»å½•çŠ¶æ€...")

            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            print(f"ğŸ” DEBUG: cookieså­—æ®µ: {list(cookies.keys())}")
            print(f"ğŸ” DEBUG: auth_tokené•¿åº¦: {len(cookies.get('auth_token', ''))}")
            print(f"ğŸ” DEBUG: ä»£ç†è®¾ç½®: {proxy_config}")

            # å…ˆæµ‹è¯•ç½‘ç»œè¿æ¥
            try:
                print("ğŸŒ æµ‹è¯•åŸºç¡€ç½‘ç»œè¿æ¥...")
                import httpx
                async with httpx.AsyncClient(proxy=proxy_config, timeout=10) as test_client:
                    resp = await test_client.get("https://httpbin.org/ip")
                    print(f"âœ… ç½‘ç»œè¿æ¥æ­£å¸¸: {resp.json()}")
            except Exception as net_e:
                print(f"âŒ ç½‘ç»œè¿æ¥å¤±è´¥: {net_e}")
                print(f"ğŸ” é”™è¯¯ç±»å‹: {type(net_e).__name__}")
                print("ğŸš¨ è¿™æ˜¯ç½‘ç»œ/ä»£ç†é—®é¢˜ï¼Œä¸æ˜¯cookiesé—®é¢˜!")
                return False

            try:
                print("ğŸ§ª æµ‹è¯•æ¨æ–‡æœç´¢...")
                tweets = await self.client.search_tweet('hello', 'Latest', count=1)

                print(f"ğŸ” DEBUG: æœç´¢ç»“æœç±»å‹: {type(tweets)}")
                print(f"ğŸ” DEBUG: æœç´¢ç»“æœé•¿åº¦: {len(tweets) if tweets else 'None'}")

                if tweets and len(tweets) > 0:
                    print("âœ… CookieséªŒè¯æˆåŠŸ! å¯ä»¥æ­£å¸¸è®¿é—®Twitter")
                    print("ğŸ‰ è¿™è¯´æ˜cookiesæ˜¯æœ‰æ•ˆçš„!")
                    print(f"ğŸ“ æ‰¾åˆ° {len(tweets)} æ¡æ¨æ–‡")

                    # æ˜¾ç¤ºç¬¬ä¸€æ¡æ¨æ–‡ä¿¡æ¯
                    first_tweet = tweets[0]
                    print(f"   ç¤ºä¾‹: {first_tweet.text[:50]}...")

                    self.user_info = {
                        'id': 'verified',
                        'screen_name': 'verified_user',
                        'name': 'Verified User',
                        'followers_count': 0,
                        'friends_count': 0
                    }
                    self.is_logged_in = True

                    # ä¿å­˜æ›´æ–°åçš„cookies
                    self.client.save_cookies(self.cookies_file)
                    return True
                else:
                    print("âŒ æœç´¢æ¨æ–‡æ— ç»“æœ")
                    print("ğŸ” å°è¯•å…¶ä»–æœç´¢è¯...")

                    # å°è¯•æœç´¢æ›´å¸¸è§çš„è¯
                    for search_term in ['twitter', 'news', 'python']:
                        print(f"ï¿½ å°è¯•æœç´¢: {search_term}")
                        try:
                            test_tweets = await self.client.search_tweet(search_term, 'Latest', count=1)
                            if test_tweets and len(test_tweets) > 0:
                                print(f"âœ… æœç´¢ '{search_term}' æˆåŠŸ! æ‰¾åˆ° {len(test_tweets)} æ¡æ¨æ–‡")
                                print("ğŸ‰ Cookiesæ˜¯æœ‰æ•ˆçš„!")
                                self.is_logged_in = True
                                return True
                        except Exception as e:
                            print(f"âŒ æœç´¢ '{search_term}' å¤±è´¥: {e}")

                    print("ğŸ¤” æ‰€æœ‰æœç´¢éƒ½æ— ç»“æœï¼Œå¯èƒ½æ˜¯Twitteré™åˆ¶æˆ–cookiesé—®é¢˜")
                    return False

            except Exception as e:
                print(f"âŒ æ¨æ–‡æœç´¢å¤±è´¥: {e}")
                print(f"ï¿½ é”™è¯¯ç±»å‹: {type(e).__name__}")

                if 'timeout' in str(e).lower() or 'ConnectTimeout' in str(e):
                    print("ğŸš¨ è¿™æ˜¯ç½‘ç»œè¿æ¥é—®é¢˜!")
                elif '401' in str(e) or '403' in str(e) or 'unauthorized' in str(e).lower():
                    print("ğŸš¨ è¿™æ˜¯cookiesè®¤è¯é—®é¢˜!")
                else:
                    print("ğŸ¤” å…¶ä»–é—®é¢˜ï¼Œå¯èƒ½æ˜¯cookiesæˆ–APIé™åˆ¶")

                return False
                
        except Exception as e:
            print(f"âŒ ç™»å½•å¤±è´¥: {e}")
            return False
    

    
    async def _get_user_info(self) -> dict:
        """è·å–ç”¨æˆ·ä¿¡æ¯ - ç®€åŒ–ç‰ˆæœ¬"""
        # ç°åœ¨è¿™ä¸ªæ–¹æ³•ä¸»è¦ç”¨äºå‘åå…¼å®¹
        return self.user_info if hasattr(self, 'user_info') and self.user_info else None
    
    async def scrape_user_tweets(self, username: str, max_tweets: int = 100, start_date: str = None, end_date: str = None, continue_from_checkpoint: bool = False) -> list:
        """æŠ“å–ç”¨æˆ·æ¨æ–‡ï¼Œæ”¯æŒæ—¶é—´èŒƒå›´å’Œæ£€æŸ¥ç‚¹"""
        if not self.is_logged_in:
            print("âŒ è¯·å…ˆç™»å½•")
            return []

        # æ£€æŸ¥ç‚¹æ–‡ä»¶å¤„ç†
        checkpoint_dir = "checkpoints"
        os.makedirs(checkpoint_dir, exist_ok=True)
        checkpoint_file = os.path.join(checkpoint_dir, f"{username}_cursor.txt")
        
        initial_cursor = None
        if continue_from_checkpoint and os.path.exists(checkpoint_file):
            with open(checkpoint_file, 'r') as f:
                initial_cursor = f.read().strip()
            print(f"â„¹ï¸ ä»æ£€æŸ¥ç‚¹åŠ è½½ cursor: {initial_cursor[:20]}...")

        try:
            s_date = datetime.strptime(start_date, '%Y-%m-%d') if start_date else None
            e_date = datetime.strptime(end_date, '%Y-%m-%d') if end_date else None

            print(f"ğŸ“‹ å¼€å§‹æŠ“å– @{username} çš„æ¨æ–‡...")
            if s_date or e_date:
                print(f"   æ—¶é—´èŒƒå›´: {start_date or '...'} -> {end_date or '...'}")

            user = await self.client.get_user_by_screen_name(username)
            # ä½¿ç”¨ initial_cursor å¼€å§‹æŠ“å–
            tweets = await self.client.get_user_tweets(user.id, 'Tweets', count=40, cursor=initial_cursor)
            tweets_data = []
            
            count = 0
            stop_scraping = False
            while tweets and count < max_tweets and not stop_scraping:
                # åœ¨æ¯æ¬¡å¾ªç¯å¼€å§‹æ—¶ï¼Œä¿å­˜å½“å‰çš„cursorä½œä¸ºæ£€æŸ¥ç‚¹
                if tweets.next_cursor:
                    with open(checkpoint_file, 'w') as f:
                        f.write(tweets.next_cursor)

                for tweet in tweets:
                    if count >= max_tweets:
                        break

                    tweet_time = tweet.created_at
                    if isinstance(tweet_time, str):
                        try:
                            tweet_time = datetime.strptime(tweet_time, '%a %b %d %H:%M:%S %z %Y')
                        except ValueError:
                            tweet_time = datetime.fromisoformat(tweet_time.replace('Z', '+00:00'))
                    
                    if s_date and tweet_time.date() < s_date.date():
                        print(f"â„¹ï¸ æ¨æ–‡ ({tweet.created_at.date()}) æ—©äºå¼€å§‹æ—¥æœŸ ({start_date})ï¼Œåœæ­¢æŠ“å–ã€‚")
                        stop_scraping = True
                        break

                    if (not s_date or tweet_time.date() >= s_date.date()) and \
                       (not e_date or tweet_time.date() <= e_date.date()):
                        
                        tweet_data = {
                            'id': tweet.id,
                            'text': tweet.text,
                            'created_at': tweet.created_at,
                            'user_name': user.name,
                            'user_screen_name': user.screen_name,
                            'retweet_count': getattr(tweet, 'retweet_count', 0),
                            'favorite_count': getattr(tweet, 'favorite_count', 0),
                            'url': f"https://twitter.com/{user.screen_name}/status/{tweet.id}"
                        }
                        tweets_data.append(tweet_data)
                        count += 1

                if stop_scraping:
                    break

                if count < max_tweets:
                    retries = 3
                    for attempt in range(retries):
                        try:
                            print(f"    ...å·²è·å– {count} æ¡æœ‰æ•ˆæ¨æ–‡ï¼Œç­‰å¾… 1.5 ç§’åç»§ç»­ç¿»é¡µ...")
                            await asyncio.sleep(1.5)
                            tweets = await tweets.next()
                            break
                        except Exception as page_e:
                            if "429" in str(page_e) or "rate limit" in str(page_e).lower():
                                wait_time = 60 * (attempt + 1)
                                print(f"ï¸ï¸ï¸âš ï¸ è§¦å‘é€Ÿç‡é™åˆ¶ï¼ç¬¬ {attempt + 1}/{retries} æ¬¡é‡è¯•ï¼Œå°†æš‚åœ {wait_time} ç§’...")
                                await asyncio.sleep(wait_time)
                            else:
                                print(f"âŒ æ— æ³•è·å–ä¸‹ä¸€é¡µï¼Œé”™è¯¯ç±»å‹: {type(page_e).__name__}, è¯¦æƒ…: {page_e}")
                                print("âš ï¸ å¯èƒ½æ˜¯å·²åˆ°è¾¾æ¨æ–‡æœ«å°¾æˆ–é‡åˆ°éé€Ÿç‡é™åˆ¶çš„APIé”™è¯¯ã€‚")
                                tweets = None
                                break
                    
                    if tweets is None:
                        break
            
            if s_date or e_date:
                final_message = f"âœ… æŠ“å–å®Œæˆï¼Œå…±è·å– {len(tweets_data)} æ¡åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ¨æ–‡"
            else:
                final_message = f"âœ… æŠ“å–å®Œæˆï¼Œå…±è·å– {len(tweets_data)} æ¡æ¨æ–‡"
                if len(tweets_data) < max_tweets:
                    final_message += f" (ç›®æ ‡ {max_tweets} æ¡ï¼Œå¯èƒ½å·²åˆ°è¾¾ç”¨æˆ·æ¨æ–‡æœ«å°¾æˆ–è§¦å‘APIé™åˆ¶)"
            
            print(final_message)
            # ä»»åŠ¡æˆåŠŸå®Œæˆåï¼Œåˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶
            if os.path.exists(checkpoint_file):
                os.remove(checkpoint_file)
                print("â„¹ï¸ ä»»åŠ¡å®Œæˆï¼Œå·²åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶ã€‚")

            return tweets_data
            
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}")
            return []
    
    def save_tweets(self, tweets_data: list, username: str, format_type: str = 'json'):
        """ä¿å­˜æ¨æ–‡æ•°æ®åˆ°æŒ‡å®šæ–‡ä»¶å¤¹"""
        if not tweets_data:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        # 1. å®šä¹‰å¹¶åˆ›å»ºä¸»è¾“å‡ºç›®å½•å’Œç”¨æˆ·å­ç›®å½•
        output_dir = "downloaded_tweets"
        user_dir = os.path.join(output_dir, username)
        os.makedirs(user_dir, exist_ok=True)  # exist_ok=True é¿å…åœ¨æ–‡ä»¶å¤¹å·²å­˜åœ¨æ—¶æŠ¥é”™

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        base_filename = f"{username}_{timestamp}"

        if format_type == 'json':
            filename = os.path.join(user_dir, f"{base_filename}.json")
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(tweets_data, f, ensure_ascii=False, indent=2, default=str)
            print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {filename}")

        elif format_type == 'txt':
            filename = os.path.join(user_dir, f"{base_filename}.txt")
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"@{username} çš„æ¨æ–‡åˆé›†\n")
                f.write(f"æŠ“å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ€»è®¡: {len(tweets_data)} æ¡æ¨æ–‡\n")
                f.write("=" * 50 + "\n\n")

                for i, tweet in enumerate(tweets_data, 1):
                    f.write(f"æ¨æ–‡ {i}:\n")
                    f.write(f"æ—¶é—´: {tweet['created_at']}\n")
                    f.write(f"å†…å®¹: {tweet['text']}\n")
                    f.write(f"é“¾æ¥: {tweet['url']}\n")
                    f.write("-" * 30 + "\n\n")

            print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {filename}")

async def interactive_mode():
    """äº¤äº’å¼æ¨¡å¼"""
    print("ğŸª æµè§ˆå™¨æˆæƒTwitteræŠ“å–å·¥å…·")
    print("=" * 50)

    cookies_file = "cookies.json"
    if not os.path.exists(cookies_file):
        print(f"\nâŒ æ‰¾ä¸åˆ° {cookies_file} æ–‡ä»¶")
        print("ğŸ“– è¯·å…ˆè¿è¡Œ: python cookie_helper.py guide")
        return
    
    scraper = BrowserAuthScraper(cookies_file)
    print(f"\nğŸ” æ­£åœ¨ç™»å½•...")
    if not await scraper.login_with_cookies():
        return
    
    print(f"\nğŸ“ è¯·è¾“å…¥æŠ“å–å‚æ•°:")
    target_user = input("ç›®æ ‡ç”¨æˆ·å: ").strip()
    if not target_user:
        print("âŒ ç”¨æˆ·åä¸èƒ½ä¸ºç©º")
        return

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ£€æŸ¥ç‚¹ï¼Œå¹¶è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
    continue_from_checkpoint = False
    checkpoint_file = f"checkpoints/{target_user}_cursor.txt"
    if os.path.exists(checkpoint_file):
        continue_choice = input(f"å‘ç° @{target_user} çš„æ£€æŸ¥ç‚¹ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ(y/N): ").strip().lower()
        if continue_choice == 'y':
            continue_from_checkpoint = True
    
    try:
        max_tweets = int(input("æœ€å¤§æ¨æ–‡æ•°é‡ (é»˜è®¤1000): ").strip() or "1000")
    except ValueError:
        max_tweets = 1000

    start_date = input("å¼€å§‹æ—¥æœŸ (YYYY-MM-DD, å¯é€‰): ").strip()
    end_date = input("ç»“æŸæ—¥æœŸ (YYYY-MM-DD, å¯é€‰): ").strip()
    
    # ä½¿ç”¨æ•°å­—é€‰æ‹©æ–‡ä»¶æ ¼å¼
    print("è¯·é€‰æ‹©è¾“å‡ºæ ¼å¼:")
    print("  1: TXT")
    print("  2: JSON (é»˜è®¤)")
    format_choice = input("è¾“å…¥é€‰é¡¹ [2]: ").strip()

    if format_choice == '1':
        format_type = 'txt'
    else:
        format_type = 'json'
    
    print(f"\nğŸš€ å¼€å§‹æŠ“å– @{target_user} çš„æ¨æ–‡...")
    tweets = await scraper.scrape_user_tweets(target_user, max_tweets, start_date, end_date, continue_from_checkpoint)
    
    if tweets:
        scraper.save_tweets(tweets, target_user, format_type)
        print(f"\nâœ… æŠ“å–å®Œæˆï¼")
    else:
        print(f"\nâš ï¸ æ²¡æœ‰è·å–åˆ°ç¬¦åˆæ¡ä»¶çš„æ¨æ–‡")

async def command_mode():
    """å‘½ä»¤è¡Œæ¨¡å¼"""
    # ç”¨æ³•: python main.py <ç”¨æˆ·å> <æ¨æ–‡æ•°é‡> [å¼€å§‹æ—¥æœŸ] [ç»“æŸæ—¥æœŸ]
    # ç¤ºä¾‹: python main.py elonmusk 1000 2023-01-01 2023-12-31
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python main.py <ç”¨æˆ·å> <æ¨æ–‡æ•°é‡> [å¼€å§‹æ—¥æœŸ] [ç»“æŸæ—¥æœŸ]")
        print("ç¤ºä¾‹: python main.py elonmusk 1000 2023-01-01 2023-12-31")
        print("æ³¨æ„: è¯·ç¡®ä¿å½“å‰ç›®å½•æœ‰ cookies.json æ–‡ä»¶")
        return

    username = sys.argv[1]
    max_tweets = int(sys.argv[2])
    start_date = sys.argv[3] if len(sys.argv) > 3 else None
    end_date = sys.argv[4] if len(sys.argv) > 4 else None
    cookies_file = "cookies.json"
    
    scraper = BrowserAuthScraper(cookies_file)
    
    if await scraper.login_with_cookies():
        tweets = await scraper.scrape_user_tweets(username, max_tweets, start_date, end_date)
        if tweets:
            # å‘½ä»¤è¡Œæ¨¡å¼é»˜è®¤ä¿å­˜ä¸ºjson
            scraper.save_tweets(tweets, username, 'json')

async def main():
    try:
        if len(sys.argv) == 1:
            # äº¤äº’å¼æ¨¡å¼
            await interactive_mode()
        else:
            # å‘½ä»¤è¡Œæ¨¡å¼
            await command_mode()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
    except Exception as e:
        print(f"âŒ ç¨‹åºé”™è¯¯: {e}")

if __name__ == "__main__":
    asyncio.run(main())
