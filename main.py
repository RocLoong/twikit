#!/usr/bin/env python3
"""
æµè§ˆå™¨æˆæƒTwitteræŠ“å–å·¥å…·
ä¸“é—¨ä¸ºæµè§ˆå™¨cookiesç™»å½•è®¾è®¡çš„ç®€åŒ–ç‰ˆæœ¬
"""

import asyncio
import json
import os
import sys
from datetime import datetime
import logging
from cookies_manager import CookiesManager

# å¤„ç†ä»£ç†è®¾ç½®
def get_proxy_config():
    """è·å–ä»£ç†é…ç½® - è‡ªåŠ¨æ£€æµ‹å¸¸ç”¨ä»£ç†"""
    # é¦–å…ˆæ£€æŸ¥ç¯å¢ƒå˜é‡
    proxy_vars = ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY', 'all_proxy', 'ALL_PROXY']

    for var in proxy_vars:
        proxy_url = os.environ.get(var)
        if proxy_url:
            print(f"ğŸ”§ æ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ä»£ç†: {var}={proxy_url}")
            # è½¬æ¢socksä»£ç†æ ¼å¼
            if proxy_url.startswith('socks://'):
                proxy_url = proxy_url.replace('socks://', 'socks5://')
                print(f"ğŸ”„ è½¬æ¢ä»£ç†æ ¼å¼: {proxy_url}")
            return proxy_url

    # å¦‚æœæ²¡æœ‰ç¯å¢ƒå˜é‡ï¼Œè‡ªåŠ¨å°è¯•å¸¸ç”¨ä»£ç†
    print("ğŸ” æœªæ£€æµ‹åˆ°ä»£ç†ç¯å¢ƒå˜é‡ï¼Œå°è¯•å¸¸ç”¨ä»£ç†...")
    common_proxies = [
        'socks5://127.0.0.1:7897',  # ä½ çš„ä»£ç†ç«¯å£
        'socks5://127.0.0.1:1080',  # å¸¸ç”¨socks5ç«¯å£
        'socks5://127.0.0.1:7890',  # Clashé»˜è®¤ç«¯å£
        'http://127.0.0.1:8080',    # å¸¸ç”¨httpä»£ç†
    ]

    for proxy in common_proxies:
        print(f"ğŸ§ª å°è¯•ä»£ç†: {proxy}")
        return proxy  # ç›´æ¥è¿”å›ç¬¬ä¸€ä¸ªï¼ˆä½ çš„7897ç«¯å£ï¼‰

    return None

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BrowserAuthScraper:
    """åŸºäºæµè§ˆå™¨æˆæƒçš„TwitteræŠ“å–å™¨"""
    
    def __init__(self, cookies_file: str = "cookies.json"):
        self.cookies_file = cookies_file
        self.client = None
        self.user_info = None
        self.is_logged_in = False
    
    async def login_with_cookies(self) -> bool:
        """ä½¿ç”¨cookiesç™»å½•"""
        from twikit import Client

        if not os.path.exists(self.cookies_file):
            print(f"âŒ Cookiesæ–‡ä»¶ä¸å­˜åœ¨: {self.cookies_file}")
            print("è¯·å…ˆè¿è¡Œ: python cookie_helper.py guide")
            return False

        try:
            print(f"ğŸª åŠ è½½cookiesæ–‡ä»¶: {self.cookies_file}")

            # ä½¿ç”¨cookiesç®¡ç†å™¨
            cookies_manager = CookiesManager(self.cookies_file)
            if not cookies_manager.load_cookies():
                return False

            cookies = cookies_manager.get_cookies()

            # åˆ›å»ºå®¢æˆ·ç«¯ - ä½¿ç”¨ç³»ç»Ÿä»£ç†
            proxy_config = get_proxy_config()
            if proxy_config:
                print(f"ğŸ”§ åˆ›å»ºå®¢æˆ·ç«¯ (ä½¿ç”¨ä»£ç†: {proxy_config})...")
                self.client = Client('en-US', proxy=proxy_config)
            else:
                print("ğŸ”§ åˆ›å»ºå®¢æˆ·ç«¯ (æ— ä»£ç†)...")
                self.client = Client('en-US')
            self.client.set_cookies(cookies)

            # è®¾ç½®CSRFå¤´
            if 'ct0' in cookies:
                self.client.http.headers['x-csrf-token'] = cookies['ct0']
                print(f"ğŸ›¡ï¸ è®¾ç½®CSRFä»¤ç‰Œ: {cookies['ct0'][:20]}...")
            
            # éªŒè¯ç™»å½•çŠ¶æ€ - ç›´æ¥å°è¯•æœç´¢æ¨æ–‡
            print("ğŸ” éªŒè¯ç™»å½•çŠ¶æ€...")

            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            print(f"ğŸ” DEBUG: cookieså­—æ®µ: {list(cookies.keys())}")
            print(f"ğŸ” DEBUG: auth_tokené•¿åº¦: {len(cookies.get('auth_token', ''))}")
            print(f"ğŸ” DEBUG: ä»£ç†è®¾ç½®: {proxy_config}")

            # å…ˆæµ‹è¯•ç½‘ç»œè¿æ¥
            try:
                print("ğŸŒ æµ‹è¯•åŸºç¡€ç½‘ç»œè¿æ¥...")
                import httpx
                async with httpx.AsyncClient(proxy=proxy_config, timeout=10) as test_client:
                    resp = await test_client.get("https://httpbin.org/ip")
                    print(f"âœ… ç½‘ç»œè¿æ¥æ­£å¸¸: {resp.json()}")
            except Exception as net_e:
                print(f"âŒ ç½‘ç»œè¿æ¥å¤±è´¥: {net_e}")
                print(f"ğŸ” é”™è¯¯ç±»å‹: {type(net_e).__name__}")
                print("ğŸš¨ è¿™æ˜¯ç½‘ç»œ/ä»£ç†é—®é¢˜ï¼Œä¸æ˜¯cookiesé—®é¢˜!")
                return False

            try:
                print("ğŸ§ª æµ‹è¯•æ¨æ–‡æœç´¢...")
                tweets = await self.client.search_tweet('hello', 'Latest', count=1)

                print(f"ğŸ” DEBUG: æœç´¢ç»“æœç±»å‹: {type(tweets)}")
                print(f"ğŸ” DEBUG: æœç´¢ç»“æœé•¿åº¦: {len(tweets) if tweets else 'None'}")

                if tweets and len(tweets) > 0:
                    print("âœ… CookieséªŒè¯æˆåŠŸ! å¯ä»¥æ­£å¸¸è®¿é—®Twitter")
                    print("ğŸ‰ è¿™è¯´æ˜cookiesæ˜¯æœ‰æ•ˆçš„!")
                    print(f"ğŸ“ æ‰¾åˆ° {len(tweets)} æ¡æ¨æ–‡")

                    # æ˜¾ç¤ºç¬¬ä¸€æ¡æ¨æ–‡ä¿¡æ¯
                    first_tweet = tweets[0]
                    print(f"   ç¤ºä¾‹: {first_tweet.text[:50]}...")

                    self.user_info = {
                        'id': 'verified',
                        'screen_name': 'verified_user',
                        'name': 'Verified User',
                        'followers_count': 0,
                        'friends_count': 0
                    }
                    self.is_logged_in = True

                    # ä¿å­˜æ›´æ–°åçš„cookies
                    self.client.save_cookies(self.cookies_file)
                    return True
                else:
                    print("âŒ æœç´¢æ¨æ–‡æ— ç»“æœ")
                    print("ğŸ” å°è¯•å…¶ä»–æœç´¢è¯...")

                    # å°è¯•æœç´¢æ›´å¸¸è§çš„è¯
                    for search_term in ['twitter', 'news', 'python']:
                        print(f"ï¿½ å°è¯•æœç´¢: {search_term}")
                        try:
                            test_tweets = await self.client.search_tweet(search_term, 'Latest', count=1)
                            if test_tweets and len(test_tweets) > 0:
                                print(f"âœ… æœç´¢ '{search_term}' æˆåŠŸ! æ‰¾åˆ° {len(test_tweets)} æ¡æ¨æ–‡")
                                print("ğŸ‰ Cookiesæ˜¯æœ‰æ•ˆçš„!")
                                self.is_logged_in = True
                                return True
                        except Exception as e:
                            print(f"âŒ æœç´¢ '{search_term}' å¤±è´¥: {e}")

                    print("ğŸ¤” æ‰€æœ‰æœç´¢éƒ½æ— ç»“æœï¼Œå¯èƒ½æ˜¯Twitteré™åˆ¶æˆ–cookiesé—®é¢˜")
                    return False

            except Exception as e:
                print(f"âŒ æ¨æ–‡æœç´¢å¤±è´¥: {e}")
                print(f"ï¿½ é”™è¯¯ç±»å‹: {type(e).__name__}")

                if 'timeout' in str(e).lower() or 'ConnectTimeout' in str(e):
                    print("ğŸš¨ è¿™æ˜¯ç½‘ç»œè¿æ¥é—®é¢˜!")
                elif '401' in str(e) or '403' in str(e) or 'unauthorized' in str(e).lower():
                    print("ğŸš¨ è¿™æ˜¯cookiesè®¤è¯é—®é¢˜!")
                else:
                    print("ğŸ¤” å…¶ä»–é—®é¢˜ï¼Œå¯èƒ½æ˜¯cookiesæˆ–APIé™åˆ¶")

                return False
                
        except Exception as e:
            print(f"âŒ ç™»å½•å¤±è´¥: {e}")
            return False
    

    
    async def _get_user_info(self) -> dict:
        """è·å–ç”¨æˆ·ä¿¡æ¯ - ç®€åŒ–ç‰ˆæœ¬"""
        # ç°åœ¨è¿™ä¸ªæ–¹æ³•ä¸»è¦ç”¨äºå‘åå…¼å®¹
        return self.user_info if hasattr(self, 'user_info') and self.user_info else None
    
    async def scrape_user_tweets(self, username: str, max_tweets: int = 100) -> list:
        """æŠ“å–ç”¨æˆ·æ¨æ–‡"""
        if not self.is_logged_in:
            print("âŒ è¯·å…ˆç™»å½•")
            return []
        
        try:
            print(f"ğŸ“‹ å¼€å§‹æŠ“å– @{username} çš„æ¨æ–‡...")
            
            # è·å–ç”¨æˆ·ä¿¡æ¯
            try:
                user = await self.client.get_user_by_screen_name(username)
            except:
                user = await self.client.get_user_by_id(username)
            
            # è·å–æ¨æ–‡
            tweets = await self.client.get_user_tweets(user.id, 'Tweets', count=20)
            tweets_data = []
            
            count = 0
            while tweets and count < max_tweets:
                for tweet in tweets:
                    if count >= max_tweets:
                        break
                    
                    tweet_data = {
                        'id': tweet.id,
                        'text': tweet.text,
                        'created_at': tweet.created_at,
                        'user_name': user.name,
                        'user_screen_name': user.screen_name,
                        'retweet_count': getattr(tweet, 'retweet_count', 0),
                        'favorite_count': getattr(tweet, 'favorite_count', 0),
                        'url': f"https://twitter.com/{user.screen_name}/status/{tweet.id}"
                    }
                    
                    tweets_data.append(tweet_data)
                    count += 1
                
                # è·å–ä¸‹ä¸€é¡µ
                if count < max_tweets:
                    try:
                        await asyncio.sleep(2)  # å®‰å…¨å»¶è¿Ÿ
                        tweets = await tweets.next()
                    except:
                        break
            
            print(f"âœ… æŠ“å–å®Œæˆï¼Œå…±è·å– {len(tweets_data)} æ¡æ¨æ–‡")
            return tweets_data
            
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}")
            return []
    
    def save_tweets(self, tweets_data: list, username: str, format_type: str = 'json'):
        """ä¿å­˜æ¨æ–‡æ•°æ®"""
        if not tweets_data:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if format_type == 'json':
            filename = f"{username}_{timestamp}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(tweets_data, f, ensure_ascii=False, indent=2, default=str)
            print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {filename}")
        
        elif format_type == 'txt':
            filename = f"{username}_{timestamp}.txt"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"@{username} çš„æ¨æ–‡åˆé›†\n")
                f.write(f"æŠ“å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ€»è®¡: {len(tweets_data)} æ¡æ¨æ–‡\n")
                f.write("=" * 50 + "\n\n")
                
                for i, tweet in enumerate(tweets_data, 1):
                    f.write(f"æ¨æ–‡ {i}:\n")
                    f.write(f"æ—¶é—´: {tweet['created_at']}\n")
                    f.write(f"å†…å®¹: {tweet['text']}\n")
                    f.write(f"é“¾æ¥: {tweet['url']}\n")
                    f.write("-" * 30 + "\n\n")
            
            print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {filename}")

async def interactive_mode():
    """äº¤äº’å¼æ¨¡å¼"""
    print("ğŸª æµè§ˆå™¨æˆæƒTwitteræŠ“å–å·¥å…·")
    print("=" * 50)

    # ç›´æ¥ä½¿ç”¨ cookies.json æ–‡ä»¶
    cookies_file = "cookies.json"

    if not os.path.exists(cookies_file):
        print(f"\nâŒ æ‰¾ä¸åˆ° {cookies_file} æ–‡ä»¶")
        print("ğŸ“– è¯·å…ˆåˆ›å»ºcookies.jsonæ–‡ä»¶ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹æŒ‡å—:")
        print("   python cookie_helper.py guide")
        print("   python cookie_helper.py sample")
        return
    
    # åˆ›å»ºæŠ“å–å™¨å¹¶ç™»å½•
    scraper = BrowserAuthScraper(cookies_file)
    
    print(f"\nğŸ” æ­£åœ¨ç™»å½•...")
    if not await scraper.login_with_cookies():
        return
    
    # æ˜¾ç¤ºç”¨æˆ·ä¿¡æ¯
    user_info = scraper.user_info
    # print(f"\nğŸ‘¤ å½“å‰ç”¨æˆ·ä¿¡æ¯:")
    # print(f"   ç”¨æˆ·å: @{user_info['screen_name']}")
    # print(f"   æ˜¾ç¤ºå: {user_info['name']}")
    # print(f"   ç²‰ä¸æ•°: {user_info['followers_count']:,}")
    # print(f"   å…³æ³¨æ•°: {user_info['friends_count']:,}")
    
    # è·å–æŠ“å–å‚æ•°
    print(f"\nğŸ“ è¯·è¾“å…¥æŠ“å–å‚æ•°:")
    target_user = input("ç›®æ ‡ç”¨æˆ·å: ").strip()
    if not target_user:
        print("âŒ ç”¨æˆ·åä¸èƒ½ä¸ºç©º")
        return
    
    try:
        max_tweets = int(input("æœ€å¤§æ¨æ–‡æ•°é‡ [100]: ").strip() or "100")
    except ValueError:
        max_tweets = 100
    
    format_type = input("è¾“å‡ºæ ¼å¼ (json/txt) [json]: ").strip() or "json"
    
    # å¼€å§‹æŠ“å–
    print(f"\nğŸš€ å¼€å§‹æŠ“å– @{target_user} çš„æ¨æ–‡...")
    tweets = await scraper.scrape_user_tweets(target_user, max_tweets)
    
    if tweets:
        scraper.save_tweets(tweets, target_user, format_type)
        print(f"\nâœ… æŠ“å–å®Œæˆï¼")
    else:
        print(f"\nâš ï¸ æ²¡æœ‰è·å–åˆ°æ¨æ–‡")

async def command_mode():
    """å‘½ä»¤è¡Œæ¨¡å¼"""
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python browser_auth_scraper.py <ç”¨æˆ·å> <æ¨æ–‡æ•°é‡>")
        print("ç¤ºä¾‹: python browser_auth_scraper.py elonmusk 100")
        print("æ³¨æ„: è¯·ç¡®ä¿å½“å‰ç›®å½•æœ‰ cookies.json æ–‡ä»¶")
        return

    username = sys.argv[1]
    max_tweets = int(sys.argv[2])
    cookies_file = "cookies.json"  # å›ºå®šä½¿ç”¨ cookies.json
    
    scraper = BrowserAuthScraper(cookies_file)
    
    if await scraper.login_with_cookies():
        tweets = await scraper.scrape_user_tweets(username, max_tweets)
        if tweets:
            scraper.save_tweets(tweets, username)

async def main():
    try:
        if len(sys.argv) == 1:
            # äº¤äº’å¼æ¨¡å¼
            await interactive_mode()
        else:
            # å‘½ä»¤è¡Œæ¨¡å¼
            await command_mode()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
    except Exception as e:
        print(f"âŒ ç¨‹åºé”™è¯¯: {e}")

if __name__ == "__main__":
    asyncio.run(main())
